/**
 * Python Runtime Code
 * ===================
 * 
 * This module contains the Python code that gets injected into the user's
 * Python environment for runtime debugging. The code is embedded as a string
 * and written to a temporary file when debugging starts.
 */

/**
 * The Python debugging runtime code
 * This gets injected into the user's Python process
 */
export const PYTHON_RUNTIME_CODE = `
"""
LangGraph Visualizer Runtime
=============================
Auto-generated debugging runtime for LangGraph Visualizer VS Code extension.
DO NOT EDIT - This file is automatically generated.
"""

import asyncio
import json
import threading
import time
import traceback
from datetime import datetime
from typing import Any, Dict, List, Optional, Callable
from functools import wraps

# WebSocket client
try:
    import websockets
    from websockets.sync.client import connect as ws_connect
    HAS_WEBSOCKETS = True
except ImportError:
    HAS_WEBSOCKETS = False
    print("Warning: websockets not installed. Run: pip install websockets")


class ExecutionState:
    """Execution state enum"""
    RUNNING = "running"
    PAUSED = "paused"
    STEPPING = "stepping"
    STOPPED = "stopped"


class TimeCapsule:
    """
    Stores execution history for replay/navigation after flow completes.
    """
    def __init__(self):
        self.steps: List[Dict] = []
        self.is_recording = False
    
    def start_recording(self, initial_input: Any):
        """Start recording a new execution"""
        self.steps = []
        self.is_recording = True
        # Record initial state - start node has empty state (before any processing)
        self.steps.append({
            "step": 0,
            "node": "__start__",
            "type": "input",
            "input": initial_input,
            "output": None,
            "state_before": {},
            "state_after": {},  # Empty state at start
            "timestamp": datetime.now().isoformat()
        })
    
    def record_node(self, node_name: str, input_data: Any, output_data: Any, 
                    state_before: Any, state_after: Any, duration: float = 0):
        """Record a node execution"""
        if not self.is_recording:
            return
        
        self.steps.append({
            "step": len(self.steps),
            "node": node_name,
            "type": "node",
            "input": input_data,
            "output": output_data,
            "state_before": state_before,
            "state_after": state_after,
            "duration": duration,
            "timestamp": datetime.now().isoformat()
        })
    
    def stop_recording(self, final_output: Any = None, error: str = None):
        """Stop recording and finalize"""
        self.is_recording = False
        # Record final state
        self.steps.append({
            "step": len(self.steps),
            "node": "__end__",
            "type": "output",
            "input": None,
            "output": final_output,
            "state_before": self.steps[-1]["state_after"] if self.steps else None,
            "state_after": final_output,
            "error": error,
            "timestamp": datetime.now().isoformat()
        })
    
    def get_history(self) -> List[Dict]:
        """Get the full execution history"""
        return self.steps
    
    def clear(self):
        """Clear the history for next run"""
        self.steps = []
        self.is_recording = False


class VisualizerRuntime:
    """
    Runtime debugging support for LangGraph Visualizer.
    Connects to the VS Code extension via WebSocket.
    """
    
    _instance: Optional['VisualizerRuntime'] = None
    
    def __init__(self, port: int = 9876, host: str = "127.0.0.1"):
        self.host = host
        self.port = port
        self.ws = None
        self.connected = False
        self.execution_state = ExecutionState.STOPPED
        self.breakpoints: set = set()
        self.current_node: Optional[str] = None
        self.execution_log: List[Dict] = []
        
        # Time Capsule for execution history
        self.time_capsule = TimeCapsule()
        self._current_state: Dict = {}  # Track current state for capsule
        self._pending_input = None  # For receiving input from UI
        
        # Threading controls
        self._pause_event = threading.Event()
        self._pause_event.set()  # Start unpaused
        self._step_event = threading.Event()
        self._stop_requested = False
        self._lock = threading.Lock()
        
        # Message receiver thread
        self._receiver_thread: Optional[threading.Thread] = None
        self._running = False
    
    @classmethod
    def get_instance(cls, port: int = 9876) -> 'VisualizerRuntime':
        """Get or create singleton instance"""
        if cls._instance is None:
            cls._instance = cls(port=port)
        return cls._instance
    
    def connect(self) -> bool:
        """Connect to VS Code extension WebSocket server"""
        if not HAS_WEBSOCKETS:
            print("Cannot connect: websockets not installed")
            return False
        
        try:
            uri = f"ws://{self.host}:{self.port}"
            self.ws = ws_connect(uri)
            self.connected = True
            self._running = True
            
            # Start receiver thread
            self._receiver_thread = threading.Thread(target=self._receive_messages, daemon=True)
            self._receiver_thread.start()
            
            # Send connected message
            self._send_message("connected", {"status": "ready"})
            print(f"LangGraph Visualizer: Connected to VS Code on port {self.port}")
            return True
            
        except Exception as e:
            print(f"LangGraph Visualizer: Failed to connect - {e}")
            self.connected = False
            return False
    
    def disconnect(self):
        """Disconnect from VS Code"""
        self._running = False
        if self.ws:
            try:
                self.ws.close()
            except:
                pass
        self.ws = None
        self.connected = False
    
    def _receive_messages(self):
        """Background thread to receive commands from VS Code"""
        while self._running and self.ws:
            try:
                message = self.ws.recv(timeout=0.1)
                if message:
                    self._handle_command(json.loads(message))
            except TimeoutError:
                continue
            except Exception as e:
                if self._running:
                    print(f"LangGraph Visualizer: Receive error - {e}")
                break
    
    def _handle_command(self, message: Dict):
        """Handle command from VS Code"""
        command = message.get("command")
        data = message.get("data", {})
        
        if command == "pause":
            self.pause()
        elif command == "resume":
            self.resume()
        elif command == "step":
            self.step()
        elif command == "stop":
            self.stop()
        elif command == "input_response":
            # Store the input for the waiting function
            self._pending_input = data.get("input")
        elif command == "set_breakpoint":
            self.add_breakpoint(data.get("nodeId"))
        elif command == "remove_breakpoint":
            self.remove_breakpoint(data.get("nodeId"))
    
    def _send_message(self, msg_type: str, data: Any):
        """Send message to VS Code"""
        if not self.connected or not self.ws:
            return
        
        message = {
            "type": msg_type,
            "timestamp": int(time.time() * 1000),
            "data": self._serialize(data)
        }
        
        try:
            self.ws.send(json.dumps(message))
        except Exception as e:
            print(f"LangGraph Visualizer: Send error - {e}")
    
    def _serialize(self, obj: Any) -> Any:
        """Serialize object for JSON"""
        if obj is None:
            return None
        if isinstance(obj, (str, int, float, bool)):
            return obj
        if isinstance(obj, (list, tuple)):
            return [self._serialize(item) for item in obj]
        if isinstance(obj, dict):
            return {str(k): self._serialize(v) for k, v in obj.items()}
        # Try to get a string representation
        try:
            return str(obj)
        except:
            return f"<{type(obj).__name__}>"
    
    # Execution control
    def start_execution(self, initial_input: Any = None):
        """Called when graph execution starts"""
        with self._lock:
            self.execution_state = ExecutionState.RUNNING
            self._stop_requested = False
            self._pause_event.set()
            self.execution_log = []
            self._current_state = initial_input if isinstance(initial_input, dict) else {}
        
        # Clear and start recording in time capsule
        self.time_capsule.clear()
        self.time_capsule.start_recording(initial_input)
        
        self._send_message("graph_start", {"status": "started"})
    
    def end_execution(self, output: Any = None, error: str = None):
        """Called when graph execution ends"""
        with self._lock:
            self.execution_state = ExecutionState.STOPPED
        
        # Stop recording and send time capsule data
        self.time_capsule.stop_recording(output, error)
        capsule_history = self.time_capsule.get_history()
        
        self._send_message("graph_end", {
            "output": output,
            "error": error,
            "log": self.execution_log,
            "timeCapsule": capsule_history
        })
    
    def pause(self):
        """Pause execution"""
        with self._lock:
            if self.execution_state == ExecutionState.RUNNING:
                self.execution_state = ExecutionState.PAUSED
                self._pause_event.clear()
        self._send_message("paused", {"node": self.current_node})
    
    def resume(self):
        """Resume execution"""
        with self._lock:
            if self.execution_state == ExecutionState.PAUSED:
                self.execution_state = ExecutionState.RUNNING
                self._pause_event.set()
        self._send_message("resumed", {})
    
    def step(self):
        """Step one node"""
        with self._lock:
            if self.execution_state == ExecutionState.PAUSED:
                self.execution_state = ExecutionState.STEPPING
                self._step_event.set()
    
    def stop(self):
        """Stop execution"""
        with self._lock:
            self._stop_requested = True
            self.execution_state = ExecutionState.STOPPED
            self._pause_event.set()
            self._step_event.set()
        self._send_message("stopped", {})
    
    def add_breakpoint(self, node_id: str):
        """Add breakpoint"""
        if node_id:
            self.breakpoints.add(node_id)
    
    def remove_breakpoint(self, node_id: str):
        """Remove breakpoint"""
        self.breakpoints.discard(node_id)
    
    def checkpoint(self, node_name: str, state_before: Any = None) -> bool:
        """
        Called before each node execution.
        Returns False if execution should stop.
        """
        self.current_node = node_name
        
        if self._stop_requested:
            return False
        
        # Check breakpoint
        if node_name in self.breakpoints and self.execution_state == ExecutionState.RUNNING:
            self._send_message("breakpoint_hit", {"node": node_name})
            self.pause()
        
        # Handle stepping
        if self.execution_state == ExecutionState.STEPPING:
            with self._lock:
                self.execution_state = ExecutionState.PAUSED
                self._step_event.clear()
                self._pause_event.clear()
            self._send_message("paused", {"node": node_name, "reason": "step"})
        
        # Wait if paused
        while not self._pause_event.is_set() and not self._stop_requested:
            if self._step_event.wait(timeout=0.1):
                self._step_event.clear()
                break
            time.sleep(0.05)
        
        return not self._stop_requested
    
    # Node tracking
    def on_node_start(self, node_name: str, input_data: Any, state: Any):
        """Called when a node starts executing"""
        self.current_node = node_name
        
        # Store state before this node
        state_before = dict(self._current_state) if self._current_state else {}
        
        log_entry = {
            "node": node_name,
            "type": "start",
            "timestamp": datetime.now().isoformat(),
            "input": input_data,
            "state_before": state_before
        }
        self.execution_log.append(log_entry)
        
        self._send_message("node_start", {
            "nodeId": node_name,
            "nodeName": node_name,
            "input": input_data,
            "stateBefore": state_before
        })
    
    def on_node_end(self, node_name: str, output_data: Any, state: Any, duration: float = 0):
        """Called when a node finishes executing"""
        # Get state before from log
        state_before = None
        for entry in reversed(self.execution_log):
            if entry.get("node") == node_name and entry.get("type") == "start":
                state_before = entry.get("state_before")
                break
        
        # Update current state with output
        state_after = state
        if isinstance(state, dict):
            self._current_state.update(state)
            state_after = dict(self._current_state)
        
        log_entry = {
            "node": node_name,
            "type": "end",
            "timestamp": datetime.now().isoformat(),
            "output": output_data,
            "state_after": state_after,
            "duration": duration
        }
        self.execution_log.append(log_entry)
        
        # Record to time capsule
        self.time_capsule.record_node(
            node_name, 
            output_data,  # input to this step
            output_data,  # output from this step
            state_before, 
            state_after, 
            duration
        )
        
        self._send_message("node_end", {
            "nodeId": node_name,
            "nodeName": node_name,
            "output": output_data,
            "stateAfter": state_after,
            "duration": duration
        })
    
    def on_state_update(self, state: Any, node_name: str = None):
        """Called when state is updated"""
        self._send_message("state_update", {
            "state": state,
            "node": node_name or self.current_node
        })
    
    def on_error(self, error: str, node_name: str = None):
        """Called when an error occurs"""
        self._send_message("error", {
            "error": error,
            "node": node_name or self.current_node,
            "traceback": traceback.format_exc()
        })


def create_visualizer_callback(runtime: VisualizerRuntime):
    """
    Create a LangGraph callback handler that reports to the visualizer.
    This works with LangGraph's RunnableConfig callbacks.
    """
    from langchain_core.callbacks import BaseCallbackHandler
    from langchain_core.outputs import LLMResult
    
    class VisualizerCallback(BaseCallbackHandler):
        """Callback handler for LangGraph visualization"""
        
        def __init__(self, runtime: VisualizerRuntime):
            self.runtime = runtime
            self._node_start_times: Dict[str, float] = {}
        
        def on_chain_start(self, serialized: Dict[str, Any], inputs: Dict[str, Any], **kwargs):
            run_id = kwargs.get("run_id")
            name = serialized.get("name", str(run_id)[:8] if run_id else "unknown")
            
            # Check if we should continue
            if not self.runtime.checkpoint(name, inputs):
                raise KeyboardInterrupt("Execution stopped by user")
            
            self._node_start_times[name] = time.time()
            # In LangGraph, inputs to a node often contain the current state
            self.runtime.on_node_start(name, inputs, inputs)
        
        def on_chain_end(self, outputs: Dict[str, Any], **kwargs):
            run_id = kwargs.get("run_id")
            name = kwargs.get("name", str(run_id)[:8] if run_id else "unknown")
            
            duration = time.time() - self._node_start_times.get(name, time.time())
            # In LangGraph, outputs from a node are the state updates
            self.runtime.on_node_end(name, outputs, outputs, duration)
        
        def on_chain_error(self, error: BaseException, **kwargs):
            self.runtime.on_error(str(error))
        
        def on_llm_start(self, serialized: Dict[str, Any], prompts: List[str], **kwargs):
            name = serialized.get("name", "LLM")
            self.runtime.on_node_start(f"llm:{name}", {"prompts": prompts}, None)
        
        def on_llm_end(self, response: LLMResult, **kwargs):
            outputs = []
            for gen in response.generations:
                for g in gen:
                    outputs.append(g.text if hasattr(g, 'text') else str(g))
            self.runtime.on_node_end("llm", {"outputs": outputs}, None, 0)
        
        def on_llm_error(self, error: BaseException, **kwargs):
            self.runtime.on_error(str(error), "llm")
        
        def on_tool_start(self, serialized: Dict[str, Any], input_str: str, **kwargs):
            name = serialized.get("name", "tool")
            self.runtime.on_node_start(f"tool:{name}", {"input": input_str}, None)
        
        def on_tool_end(self, output: str, **kwargs):
            self.runtime.on_node_end("tool", {"output": output}, None, 0)
        
        def on_tool_error(self, error: BaseException, **kwargs):
            self.runtime.on_error(str(error), "tool")
    
    return VisualizerCallback(runtime)


def wrap_graph_for_debugging(graph, runtime: VisualizerRuntime):
    """
    Wrap a compiled LangGraph for debugging.
    Uses stream to track node execution with proper timing.
    """
    original_invoke = graph.invoke
    original_ainvoke = getattr(graph, 'ainvoke', None)
    original_stream = getattr(graph, 'stream', None)
    
    def wrapped_invoke(input_data, config=None, **kwargs):
        runtime.start_execution(input_data)
        
        if config is None:
            config = {}
        
        try:
            # Send initial input
            runtime._send_message("input", {"data": input_data})
            
            # Use stream to track each node
            if original_stream:
                result = None
                current_node = None
                node_start_time = None
                
                # Stream returns chunks after each node completes
                for chunk in original_stream(input_data, config, **kwargs):
                    if isinstance(chunk, dict):
                        for node_name, node_output in chunk.items():
                            # End previous node if there was one
                            if current_node:
                                duration = time.time() - node_start_time if node_start_time else 0
                                # Use result (previous node output) for state, not next node output
                                runtime.on_node_end(current_node, result, result, duration)
                            
                            # Start this node (it's actually just completed, but we show it as "active")
                            runtime.on_node_start(node_name, node_output, node_output)
                            
                            # Keep it highlighted for a moment
                            current_node = node_name
                            node_start_time = time.time()
                            result = node_output
                            
                            # Check if we should continue
                            if not runtime.checkpoint(node_name, node_output):
                                raise KeyboardInterrupt("Execution stopped by user")
                
                # End the last node
                if current_node:
                    runtime.on_node_end(current_node, result, result, 0)
                
                # Send final output
                runtime._send_message("output", {"data": result})
                runtime.end_execution(output=result)
                return result
            else:
                # Fallback to regular invoke
                result = original_invoke(input_data, config, **kwargs)
                runtime._send_message("output", {"data": result})
                runtime.end_execution(output=result)
                return result
                
        except KeyboardInterrupt:
            runtime.end_execution(error="Stopped by user")
            raise
        except Exception as e:
            runtime.on_error(str(e))
            runtime.end_execution(error=str(e))
            raise
    
    async def wrapped_ainvoke(input_data, config=None, **kwargs):
        runtime.start_execution()
        
        if config is None:
            config = {}
        
        callbacks = config.get("callbacks", [])
        callbacks.append(create_visualizer_callback(runtime))
        config["callbacks"] = callbacks
        
        try:
            runtime._send_message("input", {"data": input_data})
            result = await original_ainvoke(input_data, config, **kwargs)
            runtime._send_message("output", {"data": result})
            runtime.end_execution(output=result)
            return result
        except Exception as e:
            runtime.on_error(str(e))
            runtime.end_execution(error=str(e))
            raise
    
    def wrapped_stream(input_data, config=None, **kwargs):
        runtime.start_execution()
        
        if config is None:
            config = {}
        
        try:
            runtime._send_message("input", {"data": input_data})
            
            # Nodes are already patched, just yield chunks
            for chunk in original_stream(input_data, config, **kwargs):
                yield chunk
            
            runtime.end_execution()
        except Exception as e:
            runtime.on_error(str(e))
            runtime.end_execution(error=str(e))
            raise
    
    # Monkey-patch the graph
    graph.invoke = wrapped_invoke
    if original_ainvoke:
        graph.ainvoke = wrapped_ainvoke
    if original_stream:
        graph.stream = wrapped_stream
    
    return graph


# Convenience function to run with debugging
def run_with_visualizer(graph, input_data, port: int = 9876, **kwargs):
    """
    Run a LangGraph with visualizer debugging enabled.
    
    Usage:
        from langgraph_visualizer_runtime import run_with_visualizer
        result = run_with_visualizer(compiled_graph, {"messages": [...]})
    """
    runtime = VisualizerRuntime.get_instance(port=port)
    
    if not runtime.connected:
        if not runtime.connect():
            print("Warning: Could not connect to VS Code. Running without debugging.")
            return graph.invoke(input_data, **kwargs)
    
    wrapped = wrap_graph_for_debugging(graph, runtime)
    return wrapped.invoke(input_data, **kwargs)


# Auto-connect on import if VS Code server is running
def auto_connect(port: int = 9876) -> Optional[VisualizerRuntime]:
    """Attempt to connect to VS Code visualizer"""
    runtime = VisualizerRuntime.get_instance(port=port)
    if runtime.connect():
        return runtime
    return None


if __name__ == "__main__":
    # Test connection
    runtime = auto_connect()
    if runtime:
        print("Connected to VS Code!")
        runtime.disconnect()
    else:
        print("Could not connect to VS Code")
`;

/**
 * Get the Python runtime code with the specified port
 */
export function getPythonRuntimeCode(port: number = 9876): string {
    return PYTHON_RUNTIME_CODE.replace(/port:\s*int\s*=\s*9876/g, `port: int = ${port}`);
}

/**
 * Generate a wrapper script that runs the user's code with debugging
 */
export function generateDebugWrapperScript(
    userScriptPath: string, 
    port: number,
    graphVariableName: string = 'compiled_graph'
): string {
    return `
# LangGraph Visualizer Debug Wrapper
# Auto-generated - DO NOT EDIT

import sys
import os

# Disable LangChain telemetry to avoid OTEL errors
os.environ["LANGCHAIN_TRACING_V2"] = "false"
os.environ["LANGCHAIN_TRACING"] = "false"
os.environ["LANGSMITH_TRACING"] = "false"
os.environ["LANGCHAIN_CALLBACKS_BACKGROUND"] = "false"

# Add the directory containing the user's script to the path
sys.path.insert(0, os.path.dirname(r"${userScriptPath}"))

# Import the runtime
${PYTHON_RUNTIME_CODE}

# Import the user's module (but don't execute __main__ yet)
import importlib.util
spec = importlib.util.spec_from_file_location("user_module", r"${userScriptPath}")
user_module = importlib.util.module_from_spec(spec)
sys.modules["user_module"] = user_module

# Store original __name__ check by reading the file
_user_script_content = open(r"${userScriptPath}").read()
_has_main_block = 'if __name__' in _user_script_content and '__main__' in _user_script_content

# Execute the module (this loads classes, functions, and module-level code, but not __main__ block)
spec.loader.exec_module(user_module)

# Get the graph from the user's module
graph = getattr(user_module, "${graphVariableName}", None)

if graph is None:
    # Try common graph variable names
    for name in ['compiled_graph', 'graph', 'app', 'workflow', 'compiled']:
        if hasattr(user_module, name):
            obj = getattr(user_module, name)
            if hasattr(obj, 'invoke'):
                graph = obj
                break

if graph is None:
    print("Error: Could not find a compiled graph in the module.")
    print("Make sure you have a variable named 'compiled_graph', 'graph', 'app', or 'workflow'")
    sys.exit(1)

# Connect to VS Code
runtime = VisualizerRuntime.get_instance(port=${port})
if not runtime.connect():
    print("Warning: Could not connect to VS Code visualizer")
    print("Make sure the debug session is started in VS Code")

# Wrap the graph for debugging - replace in user module so __main__ uses it
wrapped_graph = wrap_graph_for_debugging(graph, runtime)

# Replace the original graph in user_module with the wrapped version
for name in ['compiled_graph', 'graph', 'app', 'workflow', 'compiled']:
    if hasattr(user_module, name):
        obj = getattr(user_module, name)
        if obj is graph:
            setattr(user_module, name, wrapped_graph)

print("\\n" + "="*50)
print("LangGraph Visualizer Debug Mode Active")
print("="*50)
print(f"Connected to VS Code on port {runtime.port}")
print(f"Graph: {type(graph).__name__}")

# Patch StateGraph.compile to wrap resulting graphs
_original_compile = None
try:
    from langgraph.graph import StateGraph
    _original_compile = StateGraph.compile
    
    def _patched_compile(self, *args, **kwargs):
        compiled = _original_compile(self, *args, **kwargs)
        return wrap_graph_for_debugging(compiled, runtime)
    
    StateGraph.compile = _patched_compile
    print("Graph compilation patched for debugging")
except Exception as e:
    print(f"Warning: Could not patch StateGraph.compile: {e}")

# If the user script has a __main__ block, execute it
if _has_main_block:
    print("\\nExecuting script's main block...")
    print("="*50 + "\\n")
    
    import ast
    import textwrap
    
    script_content = open(r"${userScriptPath}").read()
    
    try:
        tree = ast.parse(script_content)
        main_body = None
        
        # Find the if __name__ == "__main__" block
        for node in ast.iter_child_nodes(tree):
            if isinstance(node, ast.If):
                # Check if this is if __name__ == "__main__":
                test = node.test
                if isinstance(test, ast.Compare):
                    left = test.left
                    if isinstance(left, ast.Name) and left.id == '__name__':
                        comparators = test.comparators
                        if len(comparators) == 1:
                            comp = comparators[0]
                            if isinstance(comp, ast.Constant) and comp.value == '__main__':
                                main_body = node.body
                                break
                            elif isinstance(comp, ast.Str) and comp.s == '__main__':
                                main_body = node.body
                                break
        
        if main_body:
            # Get source lines for the body statements
            lines = script_content.split('\\n')
            
            # Get the line range of the body
            start_line = main_body[0].lineno - 1  # 0-indexed
            end_line = main_body[-1].end_lineno if hasattr(main_body[-1], 'end_lineno') else len(lines)
            
            # Extract the body lines
            body_lines = lines[start_line:end_line]
            body_code = '\\n'.join(body_lines)
            
            # Dedent the code to remove the indentation
            body_code = textwrap.dedent(body_code)
            
            # Execute the main block with the user module's namespace
            exec_globals = dict(vars(user_module))
            exec_globals['__name__'] = '__main__'
            exec_globals['__file__'] = r"${userScriptPath}"
            exec_globals['__builtins__'] = __builtins__
            
            exec(body_code, exec_globals)
        else:
            print("Could not find main block body")
        # Request input from VS Code UI
        request_input_from_ui()
    except Exception as e:
        import traceback
        print(f"Error executing main block: {e}")
        traceback.print_exc()
        print("Requesting input from UI...")
        request_input_from_ui()
else:
    print("\\nNo main block found - requesting input from UI...")
    request_input_from_ui()

def request_input_from_ui():
    """Request input state from VS Code UI and run the graph"""
    import json
    
    # Get state fields from the graph if possible
    state_fields = []
    try:
        # Try to get state schema from the graph
        if hasattr(wrapped_graph, 'get_graph'):
            graph_def = wrapped_graph.get_graph()
            if hasattr(graph_def, 'schema') and graph_def.schema:
                for field_name, field_info in graph_def.schema.__annotations__.items():
                    field_type = str(field_info).replace("typing.", "")
                    state_fields.append({"name": field_name, "type": field_type})
    except:
        pass
    
    # If no fields found, try to get from user module's State class
    if not state_fields:
        for name in dir(user_module):
            obj = getattr(user_module, name)
            if hasattr(obj, '__annotations__') and name in ['State', 'ChatState', 'AgentState', 'GraphState']:
                for field_name, field_type in obj.__annotations__.items():
                    state_fields.append({"name": field_name, "type": str(field_type)})
                break
    
    # Send request to VS Code
    runtime._send_message("request_input", {
        "stateFields": state_fields,
        "message": "Please provide initial state values to run the graph"
    })
    
    print("Waiting for input from VS Code UI...")
    print("Please fill in the state values in the dialog and click Run.")
    
    # Wait for input response
    def wait_for_input():
        import time
        timeout = 300  # 5 minute timeout
        start = time.time()
        
        while time.time() - start < timeout:
            # Check if we received input (stored by message handler)
            if hasattr(runtime, '_pending_input') and runtime._pending_input is not None:
                input_data = runtime._pending_input
                runtime._pending_input = None
                return input_data
            time.sleep(0.5)
        
        print("Timeout waiting for input")
        return None
    
    input_data = wait_for_input()
    
    if input_data:
        print(f"\\nReceived input: {input_data}")
        print("="*50)
        print("Running graph...")
        print("="*50 + "\\n")
        
        try:
            result = wrapped_graph.invoke(input_data)
            print(f"\\n" + "="*50)
            print("Graph execution completed!")
            print(f"Result: {json.dumps(result, indent=2, default=str)}")
            print("="*50 + "\\n")
        except Exception as e:
            print(f"Error running graph: {e}")
    else:
        print("No input received. You can still run manually:")
        print("  result = wrapped_graph.invoke(your_input)")

# Restore original compile if patched
if _original_compile:
    try:
        StateGraph.compile = _original_compile
    except:
        pass

# Make wrapped_graph available
__all__ = ['wrapped_graph', 'runtime', 'graph']
`;
}

